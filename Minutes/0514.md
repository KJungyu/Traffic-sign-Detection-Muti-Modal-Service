## VLM 교통 표지판 인식 모델 개발

### 관점

**해외 여행 중 사용하는 휴대용 블랙박스**
여행 시 낯선 교통 표지판을 마주하는 상황 다수

- GPT, 네비게이션과의 차이점
- 실시간성: 별도의 입력 없이 영상 기반 자동 인식
- 블랙박스 역할: 자동 녹화

### 최종 목표

YOLO 기반 객체 탐지 + LLM 설명 + TTS 출력을 결합한 

영상 녹화 + 실시간 표지판 설명 제공형 블랙박스 서비스

### Output 예시

```
표지판: 경적 사용
↓
YOLO 탐지 → 표지판 이미지 좌표
↓
VLM 설명: "이 표지판은 경적 사용으로, 한국에서는 사라진 교통 표지판입니다. 경적을 한 번 울려야 합니다."
↓
TTS: "여기서는 경적을 한 번 울려야 합니다. 이 표지판은 한국에서 사라진 교통 표지판입니다."

```

### 시스템 구조

```
[실시간 영상 입력]
        ↓
YOLO 표지판 객체 탐지
        ↓
Bounding Box 좌표
        ↓
VLM (LLaVA / BLIP2 / 이미지 캡셔닝)
        ↓
행동 요령 + 표지판 설명 문장 생성
        ↓
[결과 출력: 텍스트 + 오디오 + 블랙박스 영상 저장]

```

### 파이프라인

1. YOLO Detection output
2. Detection 이미지 → VLM 연결
3. VLM Fine-tuning
image에 맞는 target json
4. 시연 흐름
    
    ```
    [비디오 입력]
          ↓
    [표지판 등장]
          ↓
    YOLO 탐지 (Bounding Box)
          ↓
    VLM이 이미지 설명 생성
          ↓
    행동 요령 문장 + TTS 음성 출력
    
    ```
    
5. 결과 표시 + 블랙박스 기능
- React 기반 UI로 결과 시각화
- 영상 녹화 기능
- 시간 여유가 있다면 Avatar 출력 연동

---

### 문제점

1. VLM 학습에 필요한 json 데이터셋
1800장 이상의 이미지에 대한 Target json이 필요(1800장 이상)
→ 시간상 가능할 지?
2. 모델 학습 자원 부족
LLaVA - fine-tuning H100 - 80GB 4H 학습 (github 내용)
보유 환경 - P100 16GB… → 현실적으로 어려움

### 대안

- GPT를 통해 json 파일 생성
- LLaVA 대신 이미지 캡셔닝  or 다른 VLM 모델 제시



---

### 0514 해야할 일

1. 경량 VLM 선정 (이미지 캡셔닝 or BLIP)
2. YOLO + VLM 모델 연결
3. 데이터셋 정리 및 전처리

### 데이터셋 증강 기법

- Brightness Adjustment
밝기를 -5, +5 단위로 조절
- Contrast Adjustment
- Cropping
- Grayscale
- Flipped
    - 상하좌우 반전
    - 반전 시 bounding box의 좌표(x, y축 기준)도 함께 수정
    → 반전 이미지에서도 좌표가 정확히 적용됨을 확인
    → 데이터 증강 기법으로 적합함을 검증 완료
